---
layout: archive
permalink: /research/
author_profile: true
---
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Saba Sahban's Research</title>
</head>
<body>

    <h1>Research Overview</h1>

    <p>
        I‚Äôm currently conducting research in the <strong>AUT Cloud Lab (LPDS)</strong> under the supervision of
        <strong>Dr. Momtazpour</strong>. My research primarily focuses on federated learning, distributed systems,
        cloud/edge computing, and scalable networks. I am particularly interested in developing and optimizing systems
        that facilitate collaboration between devices while preserving data privacy and reducing latency.
    </p>

    <h2>Research Interests üîç</h2>
    <ul>
        <li><strong>Federated Learning:</strong> Developing distributed learning models that can train on edge devices without sharing raw data.</li>
        <li><strong>Distributed Systems:</strong> Exploring the design and implementation of scalable systems that can run across multiple devices.</li>
        <li><strong>Cloud/Edge Computing:</strong> Optimizing resource usage in cloud environments and enabling real-time processing at the network edge.</li>
        <li><strong>IoT (Internet of Things):</strong> Researching how edge devices can collaborate effectively in distributed environments.</li>
        <li><strong>Operating Systems & Computer Networks:</strong> Investigating systems that support efficient networking and communication in distributed settings.</li>
        <li><strong>Applied Machine Learning:</strong> Applying machine learning techniques to solve real-world problems across various domains.</li>
    </ul>

    <h2>Current Projects üöÄ</h2>

    <h3><a href="https://github.com/negaranabestani/fed-flow" target="_blank">FedFlow</a></h3>
    <p>
        A federated learning framework designed to facilitate decentralized model training across edge networks. The system optimizes resource usage,
        reduces latency through split learning, and enables efficient communication between edge nodes. The framework supports multiple architectures,
        scenarios, and federated learning algorithms, allowing for flexible experimentation.
    </p>


    <h3>1. Split Learning</h3>
    <p>
        Working on implementing split learning within federated systems to reduce communication costs and enable edge devices to train models
        collaboratively while sharing minimal data.
    </p>

    <h3>2. Mobility</h3>
    <p>
        Exploring mobility in federated learning environments, focusing on how devices can dynamically connect to and migrate between edge nodes
        without compromising model performance or latency.
    </p>

    <h3>3. Semi-Decentralized Architectures</h3>
    <p>
        Studying different architectures for semi-decentralized systems to improve the scalability, resilience, and efficiency of federated learning
        networks deployed on edge devices.
    </p>

</body>
</html>




{% include base_path %}

{% for post in site.research reversed %}
  {% include archive-single.html %}
{% endfor %}